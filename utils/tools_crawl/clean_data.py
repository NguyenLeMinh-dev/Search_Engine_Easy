import pandas as pd
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm
import time
import re
import os
from PIL import Image
from io import BytesIO
import unidecode
import concurrent.futures

# ==============================================================================
# SECTION 1: CONSTANTS AND CONFIGURATION
# ==============================================================================

# --- Input/Output Files ---
INPUT_CSV = "/home/minh/Documents/SEG_project/datas/foody_cantho_with_tags.csv"
OUTPUT_CSV = "final_processed_data.csv"
IMAGE_FOLDER = "food_images"
COMMENT_CHAR_LIMIT = 400 # Gi·∫£m nh·∫π gi·ªõi h·∫°n ƒë·ªÉ ch·ª´a ch·ªó cho ng·ªØ nghƒ©a m·ªõi

# ==============================================================================
# SECTION 2: HELPER FUNCTIONS FOR DATA CLEANING
# ==============================================================================

# --- C√°c h√†m clean c∆° b·∫£n (gi·ªØ nguy√™n, ƒë√£ r·∫•t t·ªët) ---

def clean_text(text):
    if pd.isna(text): return ''
    text = str(text).strip()
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^\w\s√Ä-·ªπ,.-]', '', text)
    return text

def clean_comment_text(text):
    if pd.isna(text): return ''
    text = str(text).strip().replace('\n', '. ')
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^\w\s√Ä-·ªπ.,!?]', '', text)
    return text

def remove_accents(text):
    """Chuy·ªÉn vƒÉn b·∫£n v·ªÅ kh√¥ng d·∫•u, lowercase. H√†m n√†y s·∫Ω ƒë∆∞·ª£c d√πng nhi·ªÅu."""
    return unidecode.unidecode(str(text)).lower().strip()

def clean_price(price_str):
    if pd.isna(price_str): return None, None
    s = str(price_str).replace('ƒë', '').replace('.', '').replace(',', '').strip()
    numbers = re.findall(r'\d+', s)
    if len(numbers) == 1:
        val = float(numbers[0])
        return val, val
    elif len(numbers) >= 2:
        return float(numbers[0]), float(numbers[1])
    return None, None

def clean_rating(r):
    try:
        val = float(r)
        return round(val, 2) if val <= 10 else round(val / 10, 2)
    except (ValueError, TypeError): return None

def clean_open_close(t):
    times = re.findall(r'\d{1,2}:\d{2}', str(t))
    if len(times) >= 2:
        try:
            open_h = int(times[0].split(':')[0])
            close_h = int(times[1].split(':')[0])
            # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p qua ƒë√™m (v√≠ d·ª•: 18:00 - 02:00)
            if close_h < open_h:
                close_h += 24
            return float(open_h), float(close_h)
        except (ValueError, IndexError): return None, None
    return None, None

def clean_gps(gps_str):
    nums = re.findall(r"[-+]?\d*\.\d+|\d+", str(gps_str))
    if len(nums) >= 2: return float(nums[0]), float(nums[1])
    return None, None

def download_image_worker(args):
    url, folder, size, img_id = args
    if not isinstance(url, str) or not url.strip(): return None
    try:
        response = requests.get(url, timeout=15)
        if response.status_code == 200:
            img = Image.open(BytesIO(response.content)).convert('RGB')
            img = img.resize(size)
            filename = f"{img_id:06d}.jpg"
            path = os.path.join(folder, filename)
            img.save(path, "JPEG", quality=90)
            return path.replace("\\", "/")
    except Exception: return None
    return None

# ==============================================================================
# SECTION 3: N√ÇNG C·∫§P - H√ÄM T·∫†O NG·ªÆ NGHƒ®A
# ==============================================================================

def generate_price_semantics(price_min, price_max):
    """T·∫°o vƒÉn b·∫£n ng·ªØ nghƒ©a t·ª´ th√¥ng tin gi√° c·∫£."""
    if pd.isna(price_min):
        return ''
    if price_max <= 50000:
        return 'Gi√° r·∫ª. Ph√π h·ª£p h·ªçc sinh sinh vi√™n.'
    if price_min <= 100000 and price_max <= 250000:
        return 'Gi√° c·∫£ h·ª£p l√Ω, b√¨nh d√¢n.'
    if price_min > 250000:
        return 'Ph√¢n kh√∫c cao c·∫•p, sang tr·ªçng.'
    return 'Gi√° c·∫£ ƒëa d·∫°ng.'

def generate_time_semantics(open_h, close_h):
    """T·∫°o vƒÉn b·∫£n ng·ªØ nghƒ©a t·ª´ th·ªùi gian m·ªü c·ª≠a."""
    if pd.isna(open_h):
        return ''
    phrases = []
    if open_h <= 8:
        phrases.append('ph·ª•c v·ª• b·ªØa s√°ng')
    if open_h <= 12 and close_h >= 13:
        phrases.append('b√°n bu·ªïi tr∆∞a')
    if open_h <= 18 and close_h >= 19:
        phrases.append('b√°n bu·ªïi t·ªëi')
    if close_h >= 22:
        phrases.append('c√≥ b√°n khuya')
    if (close_h - open_h) >= 12:
        phrases.append('m·ªü c·ª≠a c·∫£ ng√†y')
    
    return 'Th·ªùi gian: ' + ', '.join(phrases) + '.' if phrases else ''

def generate_rating_semantics(rating):
    """T·∫°o vƒÉn b·∫£n ng·ªØ nghƒ©a t·ª´ ƒëi·ªÉm ƒë√°nh gi√°."""
    if pd.isna(rating):
        return ''
    if rating >= 9.0:
        return 'Ch·∫•t l∆∞·ª£ng xu·∫•t s·∫Øc, ƒë√°nh gi√° r·∫•t cao.'
    if rating >= 8.0:
        return 'Qu√°n ngon, ch·∫•t l∆∞·ª£ng t·ªët.'
    if rating >= 7.0:
        return 'ƒê·ªãa ƒëi·ªÉm kh√°, ƒë∆∞·ª£c y√™u th√≠ch.'
    return ''

# ==============================================================================
# SECTION 4: N√ÇNG C·∫§P - H√ÄM T·∫†O VƒÇN B·∫¢N CHO SEARCH
# ==============================================================================

def create_embedding_text(row):
    """
    N√ÇNG C·∫§P: T·∫°o vƒÉn b·∫£n C√ì D·∫§U cho PhoBERT (Semantic Search).
    Bao g·ªìm c√°c c·ª•m t·ª´ ng·ªØ nghƒ©a ƒë√£ ƒë∆∞·ª£c t·∫°o ra.
    """
    parts = []
    
    # 1. T√™n qu√°n
    parts.append(f"{row['name']}.")
    
    # 2. Tags (t√≠n hi·ªáu ng·ªØ nghƒ©a r√µ r√†ng nh·∫•t)
    if pd.notna(row['tags']) and row['tags']:
        parts.append(f"Th·ªÉ lo·∫°i: {row['tags']}.")
    
    # 3. Ng·ªØ nghƒ©a ƒë∆∞·ª£c t·∫°o ra (M·ªöI)
    sem_price = generate_price_semantics(row['price_min'], row['price_max'])
    if sem_price: parts.append(sem_price)
    
    sem_time = generate_time_semantics(row['open_hour'], row['close_hour'])
    if sem_time: parts.append(sem_time)
        
    sem_rating = generate_rating_semantics(row['rating'])
    if sem_rating: parts.append(sem_rating)

    # 4. B√¨nh lu·∫≠n (ƒë√£ ƒë∆∞·ª£c r√∫t g·ªçn)
    if pd.notna(row['comments']) and row['comments']:
        truncated_comments = row['comments'][:COMMENT_CHAR_LIMIT]
        if len(row['comments']) > COMMENT_CHAR_LIMIT:
            truncated_comments += "..."
        parts.append(f"M·ªôt s·ªë ƒë√°nh gi√°: {truncated_comments}")

    # 5. Th√¥ng tin ph·ª•: ƒê·ªãa ch·ªâ
    parts.append(f"ƒê·ªãa ch·ªâ t·∫°i {row['address']}.")
    
    return ' '.join(parts)

def create_bm25_text(row):
    """
    M·ªöI: T·∫°o vƒÉn b·∫£n KH√îNG D·∫§U cho BM25 (Keyword Search).
    ƒê√¢y l√† c·ªôt "Phi√™n b·∫£n D·ªØ li·ªáu Song song" ch√∫ng ta ƒë√£ th·∫£o lu·∫≠n.
    """
    parts = [
        row['name'],
        row['tags'],
        row['comments'], # D√πng b√¨nh lu·∫≠n g·ªëc ƒë·ªÉ c√≥ nhi·ªÅu t·ª´ kh√≥a
        row['address'],
        # Th√™m c√°c ng·ªØ nghƒ©a ƒë√£ t·∫°o (kh√¥ng d·∫•u) ƒë·ªÉ tƒÉng c∆∞·ªùng t·ª´ kh√≥a
        generate_price_semantics(row['price_min'], row['price_max']),
        generate_time_semantics(row['open_hour'], row['close_hour']),
        generate_rating_semantics(row['rating'])
    ]
    
    full_text = ' '.join([str(p) for p in parts if pd.notna(p) and p])
    return remove_accents(full_text) # S·ª≠ d·ª•ng h√†m remove_accents

# ==============================================================================
# SECTION 5: MAIN PROCESSING FUNCTION (ƒê√É C·∫¨P NH·∫¨T)
# ==============================================================================

def main():
    """
    Main function to run the entire data cleaning and processing pipeline.
    """
    # --- 1. Read the source CSV file ---
    try:
        df = pd.read_csv(INPUT_CSV)
        print(f"üìñ ƒê√£ ƒë·ªçc {len(df)} d√≤ng t·ª´ '{INPUT_CSV}'.")
    except FileNotFoundError:
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file '{INPUT_CSV}'. Vui l√≤ng ch·∫°y script 'patch_tags_scraper.py' tr∆∞·ªõc.")
        return

    # --- 2. Clean and Standardize Data ---
    print("‚ú® B·∫Øt ƒë·∫ßu l√†m s·∫°ch v√† chu·∫©n h√≥a d·ªØ li·ªáu...")
    df['name'] = df['name'].apply(clean_text)
    df['address'] = df['address'].apply(clean_text)
    df['rating'] = df['rating'].apply(clean_rating)
    df['comments'] = df['comments'].apply(clean_comment_text)
    df['tags'] = df['tags'].apply(clean_text)

    df[['price_min', 'price_max']] = df['price'].apply(lambda x: pd.Series(clean_price(x)))
    df[['open_hour', 'close_hour']] = df['open_close'].apply(lambda x: pd.Series(clean_open_close(x)))
    df[['gps_lat', 'gps_long']] = df['gps'].apply(lambda x: pd.Series(clean_gps(x)))

    # --- 3. Download Images in Parallel ---
    df.dropna(subset=['name', 'image_src'], inplace=True)
    df = df.reset_index(drop=True)
    
    print(f"üñºÔ∏è  ƒêang t·∫£i {len(df)} ·∫£nh (ch·∫°y song song)...")
    os.makedirs(IMAGE_FOLDER, exist_ok=True)
    tasks = [(row['image_src'], IMAGE_FOLDER, (224, 224), i + 1) for i, row in df.iterrows()]
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        results = list(tqdm(executor.map(download_image_worker, tasks), total=len(tasks), desc="Downloading images"))
    df['image_path'] = results
    
    df.dropna(subset=['image_path'], inplace=True)
    df = df.reset_index(drop=True)
    print(f"‚úÖ ƒê√£ t·∫£i th√†nh c√¥ng {len(df)} ·∫£nh.")

    # --- 4. C·∫¨P NH·∫¨T: Create Enriched Text Columns ---
    print("‚úçÔ∏è  T·∫°o c√°c c·ªôt vƒÉn b·∫£n gi√†u ng·ªØ c·∫£nh (N√¢ng c·∫•p)...")
    
    # 4a. T·∫°o c·ªôt 'text_for_embedding' (C√ì D·∫§U, gi√†u ng·ªØ nghƒ©a)
    df['text_for_embedding'] = df.apply(create_embedding_text, axis=1)
    
    # 4b. T·∫°o c·ªôt 'text_for_bm25' (KH√îNG D·∫§U, ƒë·∫ßy ƒë·ªß t·ª´ kh√≥a)
    df['text_for_bm25'] = df.apply(create_bm25_text, axis=1)

    # 4c. T·∫°o c·ªôt 'name_no_accent' (d√πng cho g·ª£i √Ω ho·∫∑c t√¨m ki·∫øm nhanh)
    df['name_no_accent'] = df['name'].apply(remove_accents)
    
    # --- 5. Tr√≠ch xu·∫•t th√¥ng tin ph·ª• ---
    def extract_district(address):
        match = re.search(r'Qu·∫≠n\s+([\w\s]+)', str(address), re.IGNORECASE)
        return match.group(1).strip() if match else 'Kh√°c'
    df['district'] = df['address'].apply(extract_district)
    df['city'] = 'C·∫ßn Th∆°'

    # --- 6. Finalize and Save ---
    df['id'] = [f"{i:06d}" for i in range(1, len(df) + 1)]
    final_cols = [
        'id', 'name', 'name_no_accent', 'tags', 'address', 'district', 'city', 'rating',
        'price_min', 'price_max', 'open_hour', 'close_hour', 'gps_lat', 'gps_long',
        'text_for_embedding', # C·ªôt cho PhoBERT (c√≥ d·∫•u)
        'text_for_bm25',      # C·ªôt cho BM25 (kh√¥ng d·∫•u)
        'image_path', 'comments', 'url'
    ]
    
    existing_cols = [col for col in final_cols if col in df.columns]
    df_final = df[existing_cols]

    df_final.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')
    print(f"\nüéâ HO√ÄN T·∫§T! {len(df_final)} d√≤ng d·ªØ li·ªáu s·∫°ch ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i '{OUTPUT_CSV}'")

# ==============================================================================
# SECTION 6: SCRIPT EXECUTION
# ==============================================================================

if __name__ == "__main__":
    main()