import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModel
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
from tqdm import tqdm
import os
import argparse

# ==============================================================================
# SECTION 1: C·∫§U H√åNH TRUNG T√ÇM (ƒê·ªÇ ƒê√ÅNH GI√Å)
# ==============================================================================

# *** THAY ƒê·ªîI MODEL T·∫†I ƒê√ÇY ***
# Ch·ªâ c·∫ßn thay ƒë·ªïi d√≤ng n√†y, c√°c file output s·∫Ω t·ª± ƒë·ªông c·∫≠p nh·∫≠t t√™n.
MODEL_NAME = "bkai-foundation-models/vietnamese-bi-encoder"
# V√≠ d·ª• thay ƒë·ªïi:
# MODEL_NAME = "vinai/phobert-large"
# MODEL_NAME = "nguyenvulebinh/envibert" 

# --- C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n ---
# (Gi·ªØ nguy√™n ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi t·ª´ file c·ªßa b·∫°n)
BASE_DATA_PATH = "/home/minh/Documents/SEG_project/datas/datas_crawl/"
INPUT_CSV = os.path.join(BASE_DATA_PATH, "final_processed_data.csv")

# --- T·ª± ƒë·ªông t·∫°o t√™n file output d·ª±a tr√™n MODEL_NAME ---
# L·∫•y ph·∫ßn cu·ªëi c·ªßa t√™n model l√†m slug (v√≠ d·ª•: "phobert-base-v2")
model_slug = MODEL_NAME.split('/')[-1]
# File .pt ƒë·ªÉ l∆∞u tr·ªØ tensor ƒë√£ tokenize
OUTPUT_TENSORS_PATH = os.path.join(BASE_DATA_PATH, f"{model_slug}.pt")
# File .npy ƒë·ªÉ l∆∞u tr·ªØ embeddings cu·ªëi c√πng
OUTPUT_EMBEDDINGS_PATH = os.path.join(BASE_DATA_PATH, f"{model_slug}.npy")

# --- C·∫•u h√¨nh Tokenizer ---
MAX_LENGTH = 256
TOKENIZE_BATCH_SIZE = 32   # Batch size cho vi·ªác tokenize

# --- C·∫•u h√¨nh Embedding ---
EMBED_BATCH_SIZE = 128 # Batch size ƒë·ªÉ t·∫°o embedding (tƒÉng cho GPU m·∫°nh)

# ==============================================================================
# SECTION 2: H√ÄM TOKENIZE (T·ª™ Tokenize.py)
# ==============================================================================

def tokenize_for_phobert():
    """
    Ch·∫°y pipeline tokenization.
    ƒê·ªçc t·ª´ file CSV v√† l∆∞u k·∫øt qu·∫£ ra file .pt.
    """
    # --- 1. Load the clean dataset ---
    try:
        df = pd.read_csv(INPUT_CSV)
        print(f"üìñ ƒê√£ ƒë·ªçc {len(df)} d√≤ng t·ª´ '{INPUT_CSV}'.")
    except FileNotFoundError:
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file '{INPUT_CSV}'. Vui l√≤ng ch·∫°y script 'clean_data.py' tr∆∞·ªõc.")
        return False # Tr·∫£ v·ªÅ False n·∫øu th·∫•t b·∫°i

    if 'text_for_embedding' not in df.columns:
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y c·ªôt 'text_for_embedding' trong file CSV.")
        return False

    # --- 2. Load the PhoBERT tokenizer ---
    print(f"ü§ñ ƒêang t·∫£i Tokenizer cho model ('{MODEL_NAME}')...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)
        print("‚úÖ T·∫£i tokenizer th√†nh c√¥ng.")
    except Exception as e:
        print(f"‚ùå L·ªói khi t·∫£i tokenizer: {e}. Vui l√≤ng ki·ªÉm tra k·∫øt n·ªëi m·∫°ng.")
        return False

    # --- 3. Tokenize the text in batches ---
    texts = df['text_for_embedding'].tolist()
    print(f"\n‚öôÔ∏è  B·∫Øt ƒë·∫ßu tokenize {len(texts)} d√≤ng vƒÉn b·∫£n (Batch size = {TOKENIZE_BATCH_SIZE})...")
    
    all_input_ids = []
    all_attention_masks = []

    for i in tqdm(range(0, len(texts), TOKENIZE_BATCH_SIZE), desc="Tokenizing batches"):
        batch_texts = texts[i:i + TOKENIZE_BATCH_SIZE]
        
        tokenized_batch = tokenizer(
            batch_texts,
            padding='max_length',
            truncation=True,
            max_length=MAX_LENGTH,
            return_tensors='pt'
        )
        
        all_input_ids.append(tokenized_batch['input_ids'])
        all_attention_masks.append(tokenized_batch['attention_mask'])

    input_ids = torch.cat(all_input_ids, dim=0)
    attention_mask = torch.cat(all_attention_masks, dim=0)

    tokenized_output = {
        'input_ids': input_ids,
        'attention_mask': attention_mask
    }
    print("‚úÖ Tokenize ho√†n t·∫•t!")

    # --- 4. Save the results ---
    input_ids_shape = tokenized_output['input_ids'].shape
    print(f"\nK√≠ch th∆∞·ªõc c·ªßa tensor 'input_ids': {input_ids_shape}")

    try:
        torch.save(tokenized_output, OUTPUT_TENSORS_PATH)
        print(f"\nüíæ ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o file: '{OUTPUT_TENSORS_PATH}'")
        print(f"üëâ B∆∞·ªõc ti·∫øp theo: D√πng file n√†y ƒë·ªÉ ch·∫°y 'generate_embeddings'.")
        return True # Tr·∫£ v·ªÅ True n·∫øu th√†nh c√¥ng
    except Exception as e:
        print(f"‚ùå L·ªói khi l∆∞u file: {e}")
        return False

# ==============================================================================
# SECTION 3: H√ÄM T·∫†O EMBEDDING (T·ª™ generate_embedding.py)
# ==============================================================================

def generate_embeddings():
    """
    Ch·∫°y pipeline t·∫°o embedding.
    ƒê·ªçc t·ª´ file .pt v√† l∆∞u k·∫øt qu·∫£ ra file .npy.
    """
    # --- 1. Setup device (GPU/CPU) ---
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"‚úÖ S·ª≠ d·ª•ng thi·∫øt b·ªã: {device}")
    if device.type == 'cpu':
        print("‚ö†Ô∏è  C·∫£nh b√°o: Ch·∫°y tr√™n CPU s·∫Ω ch·∫≠m h∆°n ƒë√°ng k·ªÉ.")

    # --- 2. Load tokenized data ---
    try:
        # S·ª≠ d·ª•ng ƒë∆∞·ªùng d·∫´n OUTPUT_TENSORS_PATH ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh
        tokenized_data = torch.load(OUTPUT_TENSORS_PATH)
        dataset = TensorDataset(tokenized_data['input_ids'], tokenized_data['attention_mask'])
        dataloader = DataLoader(dataset, batch_size=EMBED_BATCH_SIZE, shuffle=False)
        print(f"üíæ ƒê√£ t·∫£i d·ªØ li·ªáu ƒë√£ tokenize t·ª´ '{OUTPUT_TENSORS_PATH}'.")
    except FileNotFoundError:
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file '{OUTPUT_TENSORS_PATH}'.")
        print("üëâ Vui l√≤ng ch·∫°y b∆∞·ªõc 'tokenize' tr∆∞·ªõc: python your_script_name.py --step tokenize")
        return False

    # --- 3. Load the Model ---
    print(f"ü§ñ ƒêang t·∫£i m√¥ h√¨nh ('{MODEL_NAME}')...")
    try:
        model = AutoModel.from_pretrained(MODEL_NAME).to(device)
        model.eval()
        print("‚úÖ T·∫£i m√¥ h√¨nh th√†nh c√¥ng.")
    except Exception as e:
        print(f"‚ùå L·ªói khi t·∫£i m√¥ h√¨nh: {e}. Vui l√≤ng ki·ªÉm tra k·∫øt n·ªëi m·∫°ng.")
        return False
        
    # --- 4. Generate Embeddings in Batches ---
    all_embeddings = []
    print(f"\n‚öôÔ∏è  B·∫Øt ƒë·∫ßu t·∫°o embeddings v·ªõi batch size = {EMBED_BATCH_SIZE}...")
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Generating Embeddings"):
            b_input_ids, b_attention_mask = [b.to(device) for b in batch]
            
            outputs = model(input_ids=b_input_ids, attention_mask=b_attention_mask)
            
            # L·∫•y [CLS] token embedding
            cls_embeddings = outputs.last_hidden_state[:, 0, :]
            
            all_embeddings.append(cls_embeddings.cpu().numpy())

    final_embeddings = np.concatenate(all_embeddings, axis=0)
    print("‚úÖ T·∫°o embeddings ho√†n t·∫•t!")
    print(f"K√≠ch th∆∞·ªõc c·ªßa ma tr·∫≠n embeddings: {final_embeddings.shape}")

    # --- 5. Save the final embeddings ---
    try:
        # S·ª≠ d·ª•ng ƒë∆∞·ªùng d·∫´n OUTPUT_EMBEDDINGS_PATH ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh
        np.save(OUTPUT_EMBEDDINGS_PATH, final_embeddings)
        print(f"\nüíæ ƒê√£ l∆∞u embeddings v√†o file: '{OUTPUT_EMBEDDINGS_PATH}'")
        print("üëâ B∆∞·ªõc ti·∫øp theo: D√πng file n√†y ƒë·ªÉ ch·∫°y 'search_engine.py'.")
        return True
    except Exception as e:
        print(f"‚ùå L·ªói khi l∆∞u file: {e}")
        return False

# ==============================================================================
# SECTION 4: SCRIPT EXECUTION (Tr√¨nh ƒëi·ªÅu khi·ªÉn Pipeline)
# ==============================================================================

if __name__ == "__main__":
    # Th√™m tr√¨nh ph√¢n t√≠ch ƒë·ªëi s·ªë ƒë·ªÉ ch·ªçn b∆∞·ªõc ch·∫°y
    parser = argparse.ArgumentParser(
        description=f"Pipeline Tokenize v√† T·∫°o Embedding cho model {MODEL_NAME}."
    )
    parser.add_argument(
        '--step', 
        type=str, 
        default='all', 
        choices=['all', 'tokenize', 'embed'],
        help="B∆∞·ªõc c·∫ßn ch·∫°y: 'tokenize' (ch·ªâ tokenize), 'embed' (ch·ªâ t·∫°o embedding), ho·∫∑c 'all' (c·∫£ hai, m·∫∑c ƒë·ªãnh)."
    )
    args = parser.parse_args()

    print(f"üöÄ B·∫ÆT ƒê·∫¶U PIPELINE CHO MODEL: {MODEL_NAME}")
    print(f"   Input CSV: {INPUT_CSV}")
    print(f"   Output Tensors: {OUTPUT_TENSORS_PATH}")
    print(f"   Output Embeddings: {OUTPUT_EMBEDDINGS_PATH}")
    
    if args.step in ['all', 'tokenize']:
        print("\n" + "="*50)
        print("B∆Ø·ªöC 1: TOKENIZE D·ªÆ LI·ªÜU")
        print("="*50)
        success_tokenize = tokenize_for_phobert()
        if not success_tokenize:
            print("‚ùå D·ª´ng pipeline do l·ªói ·ªü b∆∞·ªõc Tokenize.")
            exit() # Tho√°t n·∫øu b∆∞·ªõc 1 l·ªói
            
    if args.step in ['all', 'embed']:
        print("\n" + "="*50)
        print("B∆Ø·ªöC 2: T·∫†O EMBEDDINGS")
        print("="*50)
        success_embed = generate_embeddings()
        if not success_embed:
            print("‚ùå D·ª´ng pipeline do l·ªói ·ªü b∆∞·ªõc Embedding.")
            exit() # Tho√°t n·∫øu b∆∞·ªõc 2 l·ªói

    print("\nüéâ Pipeline ho√†n t·∫•t th√†nh c√¥ng! üéâ")