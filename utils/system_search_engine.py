import pandas as pd
import numpy as np
import faiss
from rank_bm25 import BM25Okapi
from transformers import AutoTokenizer, AutoModel
import torch
import os
import re
# ==============================================================================
# SECTION 1: CONSTANTS AND CONFIGURATION
# ==============================================================================

# --- File Paths ---
# ÄÆ°á»ng dáº«n nÃ y lÃ  tÆ°Æ¡ng Ä‘á»‘i so vá»›i nÆ¡i báº¡n cháº¡y 'app.py'
# Giáº£ sá»­ 'final_processed_data.csv' vÃ  'embeddings.npy' náº±m cÃ¹ng cáº¥p vá»›i 'app.py'
PROCESSED_DATA_CSV = "./datas/datas_crawl/final_processed_data.csv"
EMBEDDINGS_FILE = "./datas/datas_crawl/3epochs.npy"

# --- Model Configuration ---
MODEL_NAME = "./datas/datas_crawl/fintune_sbert_3epochs"

# --- Search Hyperparameters ---
TOP_K = 100
CANDIDATE_POOL_SIZE = 100
SCORE_THRESHOLD = 0.015

# --- Re-ranking Weights ---
RETRIEVAL_WEIGHT = 0.7
RERANK_WEIGHT = 0.3

# --- Image URL Base ---
# URL nÃ y pháº£i khá»›p vá»›i cÃ¡ch 'app.py' cháº¡y
BASE_IMAGE_URL = "http://127.0.0.1:5000/images/"


# ==============================================================================
# SECTION 2: THE ADVANCED SEARCH ENGINE CLASS
# ==============================================================================

def sanitize_query_for_filename(raw_query_text):
    """
    HÃ m nÃ y láº¥y text thÃ´ vÃ  tráº£ vá» pháº§n tÃªn file (khÃ´ng cÃ³ prefix).
    Logic pháº£i GIá»NG Há»†T hÃ m chuáº©n hÃ³a á»Ÿ file normalize.
    """
    
    # 1. XÃ³a khoáº£ng tráº¯ng thá»«a á»Ÿ 2 Ä‘áº§u
    sanitized = raw_query_text.strip()
    
    # 2. Thay khoáº£ng tráº¯ng báº±ng gáº¡ch dÆ°á»›i
    sanitized = sanitized.replace(' ', '_')
    
    # 3. Loáº¡i bá» Táº¤T Cáº¢ cÃ¡c kÃ½ tá»± Ä‘áº·c biá»‡t
    sanitized = re.sub(r'[^\w_]', '', sanitized)
    
    # 4. Gá»™p nhiá»u gáº¡ch dÆ°á»›i liÃªn tiáº¿p
    sanitized = re.sub(r'__+', '_', sanitized)
    
    # 5. (Sá»¬A Lá»–I) XÃ³a gáº¡ch dÆ°á»›i á»Ÿ Ä‘áº§u hoáº·c cuá»‘i
    sanitized = sanitized.strip('_')
    
    return sanitized

class SearchEngine:
    def __init__(self):
        print("ğŸš€ Khá»Ÿi táº¡o Search Engine (Kiáº¿n trÃºc 2 giai Ä‘oáº¡n)...")
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"âœ… Sá»­ dá»¥ng thiáº¿t bá»‹: {self.device}")

        self._load_dependencies()
        self._build_indexes()
        
        print("âœ… Search Engine Ä‘Ã£ sáºµn sÃ ng!")

    def _load_dependencies(self):
        print("ğŸ’¾ Äang táº£i dá»¯ liá»‡u vÃ  mÃ´ hÃ¬nh...")
        if not os.path.exists(PROCESSED_DATA_CSV):
            raise FileNotFoundError(f"Lá»—i: KhÃ´ng tÃ¬m tháº¥y file '{PROCESSED_DATA_CSV}'. Äáº£m báº£o nÃ³ náº±m cÃ¹ng cáº¥p vá»›i app.py.")
        if not os.path.exists(EMBEDDINGS_FILE):
             raise FileNotFoundError(f"Lá»—i: KhÃ´ng tÃ¬m tháº¥y file '{EMBEDDINGS_FILE}'. Äáº£m báº£o nÃ³ náº±m cÃ¹ng cáº¥p vá»›i app.py.")

        self.df = pd.read_csv(PROCESSED_DATA_CSV, dtype={'id': str})
        self.embeddings = np.load(EMBEDDINGS_FILE).astype('float32')
        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)
        self.model = AutoModel.from_pretrained(MODEL_NAME).to(self.device)
        self.model.eval()

    def _build_indexes(self):
        print("... âš™ï¸  Äang xÃ¢y dá»±ng FAISS index (Giai Ä‘oáº¡n 1)...")
        d = self.embeddings.shape[1]
        self.faiss_index = faiss.IndexFlatL2(d)
        if self.device.type == 'cuda':
            res = faiss.StandardGpuResources()
            self.faiss_index = faiss.index_cpu_to_gpu(res, 0, self.faiss_index)
        self.faiss_index.add(self.embeddings)

        print("... âš™ï¸  Äang xÃ¢y dá»±ng BM25 index (Giai Ä‘oáº¡n 1)...")
        corpus = self.df['text_for_embedding'].fillna('').tolist()
        tokenized_corpus = [doc.split(" ") for doc in corpus]
        self.bm25_index = BM25Okapi(tokenized_corpus)
        print("âœ… CÃ¡c chá»‰ má»¥c Retrieval Ä‘Ã£ sáºµn sÃ ng.")

    def _encode_query(self, query_text):
        with torch.no_grad():
            inputs = self.tokenizer(query_text, return_tensors="pt", padding=True, truncation=True, max_length=256).to(self.device)
            outputs = self.model(**inputs)
            return outputs.last_hidden_state[:, 0, :]

    def search(self, query):
        # print(f"\nğŸ” Äang tÃ¬m kiáº¿m cho truy váº¥n: '{query}'")
        
        # STAGE 1: RETRIEVAL
        # print(f"    -> Giai Ä‘oáº¡n 1: Thu tháº­p á»©ng viÃªn...")
        query_embedding_gpu = self._encode_query(query)
        query_embedding_cpu = query_embedding_gpu.cpu().numpy()
        
        _, semantic_indices = self.faiss_index.search(query_embedding_cpu, CANDIDATE_POOL_SIZE)
        
        bm25_tokens = query.lower().split()
        bm25_scores = self.bm25_index.get_scores(bm25_tokens)
        keyword_indices = np.argsort(bm25_scores)[::-1][:CANDIDATE_POOL_SIZE]
        
        candidate_indices = np.union1d(semantic_indices[0], keyword_indices)
        
        if len(candidate_indices) == 0:
            return pd.DataFrame()

        # STAGE 2: RE-RANKING
        # print(f"    -> Giai Ä‘oáº¡n 2: TÃ¡i xáº¿p háº¡ng {len(candidate_indices)} á»©ng viÃªn...")
        candidate_embeddings = torch.from_numpy(self.embeddings[candidate_indices]).to(self.device)
        
        query_norm = torch.nn.functional.normalize(query_embedding_gpu, p=2, dim=1)
        candidates_norm = torch.nn.functional.normalize(candidate_embeddings, p=2, dim=1)
        rerank_scores = torch.mm(query_norm, candidates_norm.transpose(0, 1)).flatten().cpu().numpy()

        rank_df = pd.DataFrame({
            'id': candidate_indices,
            'bm25': bm25_scores[candidate_indices],
            'rerank': rerank_scores
        })

        rank_df['bm25_norm'] = (rank_df['bm25'] - rank_df['bm25'].min()) / (rank_df['bm25'].max() - rank_df['bm25'].min() + 1e-6)
        rank_df['final_score'] = (RETRIEVAL_WEIGHT * rank_df['bm25_norm']) + (RERANK_WEIGHT * rank_df['rerank'])

        top_indices = rank_df.sort_values('final_score', ascending=False).head(TOP_K)['id'].values
        
        # --- Format Output ---
        results_df = self.df.iloc[top_indices].copy()
        final_scores_df = rank_df[rank_df['id'].isin(top_indices)].set_index('id')
        
        # === Sá»¬A Lá»–I 1: Sá»¬A Lá»–I .JOIN() GÃ‚Y TREO ===
        # XÃ³a 'on=...' Ä‘á»ƒ join dá»±a trÃªn index
        results_df = results_df.join(final_scores_df) 

        # === Sá»¬A Lá»–I 2: Táº O Cá»˜T Má»šI CHO FRONTEND ===
        
        # 2a. Táº¡o cá»™t 'gps' tá»« 'gps_lat' vÃ  'gps_long'
        if 'gps_lat' in results_df.columns and 'gps_long' in results_df.columns:
            results_df['gps'] = results_df['gps_lat'].astype(str) + ',' + results_df['gps_long'].astype(str)
            results_df['gps'] = results_df['gps'].replace('nan,nan', np.nan)
        else:
            print("Cáº¢NH BÃO: KhÃ´ng tÃ¬m tháº¥y cá»™t 'gps_lat' hoáº·c 'gps_long'.")
            results_df['gps'] = np.nan

        # 2b. Táº¡o cá»™t 'image_src' (URL Ä‘áº§y Ä‘á»§) tá»« 'image_path'
        # ...
        if 'image_path' in results_df.columns:
            results_df['image_src'] = results_df['image_path'].apply(
                # os.path.basename sáº½ láº¥y ra pháº§n cuá»‘i cÃ¹ng cá»§a Ä‘Æ°á»ng dáº«n (tÃªn file)
                # VÃ­ dá»¥: 'food_images/000641.jpg' -> '000641.jpg'
                lambda x: f"{BASE_IMAGE_URL}{os.path.basename(x)}" if pd.notna(x) else np.nan
            )
      
        else:
            print("Cáº¢NH BÃO: KhÃ´ng tÃ¬m tháº¥y cá»™t 'image_path'.")
            results_df['image_src'] = np.nan

        # --- Láº¥y cÃ¡c cá»™t cuá»‘i cÃ¹ng mÃ  frontend cáº§n ---
        required_cols = ['id', 'name', 'comments', 'text_for_embedding', 'gps', 'image_src', 'score','address']
        
        available_cols = [col for col in required_cols if col in results_df.columns]
        
        return results_df[available_cols].rename(columns={'final_score': 'score'})

# (HÃ m main() dÃ¹ng Ä‘á»ƒ test, giá»¯ nguyÃªn)
def main():
    try:
        engine = SearchEngine()
        test_queries = [
            "gá»i cuá»‘n tÃ´m thá»‹t", "NÆ°á»›c Ã©p trÃ¡i cÃ¢y á»Ÿ Ä‘Ã¢u lÃ  nguyÃªn cháº¥t nháº¥t, khÃ´ng pha thÃªm Ä‘Æ°á»ng vÃ  nÆ°á»›c", "cÆ¡m táº¥m sÆ°á»n bÃ¬ cháº£", "sinh tá»‘ bÆ¡", "bÃºn bÃ² huáº¿", "cÃ  phÃª trá»©ng", "láº©u thÃ¡i tomyum", "mÃ³n ngon quáº­n 1", "Ä‘á»“ Äƒn váº·t ship Ä‘Ãªm", "buffet nÆ°á»›ng", "Gá»£i Ã½ quÃ¡n Äƒn trÆ°a vÄƒn phÃ²ng cÃ³ thá»±c Ä‘Æ¡n thay Ä‘á»•i theo ngÃ y vÃ  giao hÃ ng miá»…n phÃ­.", "Ä‘á»“ Äƒn nhanh (fast food)", "Ä‘á»“ Äƒn váº·t há»c sinh", "nem nÆ°á»›ng nha trang", "trÃ  sá»¯a full topping", "háº£i sáº£n tÆ°Æ¡i sá»‘ng", "QuÃ¡n Äƒn nÃ o á»Ÿ Cáº§n ThÆ¡ Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ cao vá» vá»‡ sinh an toÃ n thá»±c pháº©m", "quÃ¡n Äƒn gia Ä‘Ã¬nh cuá»‘i tuáº§n", "mÃ³n thÃ¡i cay", "Ä‘á»“ Äƒn healthy", "gÃ  rÃ¡n kfc", "cÆ¡m chay", "quÃ¡n yÃªn tÄ©nh há»c bÃ i", "Giá»¯a nem nÆ°á»›ng CÃ¡i RÄƒng vÃ  bÃ¡nh cá»‘ng thÃ¬ mÃ³n nÃ o nÃªn thá»­ khi Ä‘áº¿n Cáº§n ThÆ¡?", "phá»Ÿ bÃ² truyá»n thá»‘ng", "quÃ¡n cÆ¡m vÄƒn phÃ²ng", "hamburger bÃ² phÃ´ mai", "MÃ¬nh muá»‘n tÃ¬m quÃ¡n há»§ tiáº¿u khÃ´ Sa ÄÃ©c chÃ­nh gá»‘c táº¡i Cáº§n ThÆ¡, báº¡n cÃ³ gá»£i Ã½ khÃ´ng", "mÃ¬ quáº£ng Ä‘áº·c biá»‡t", "nhÃ  hÃ ng sang trá»ng", "chÃ¨ ba mÃ u", "bÃ¡nh xÃ¨o miá»n tÃ¢y", "quÃ¡n Äƒn gia Ä‘Ã¬nh áº¥m cÃºng", "bÃ¡nh mÃ¬", "pizza háº£i sáº£n", "MÃ¬nh muá»‘n thá»­ cÃ¡c mÃ³n Äƒn ThÃ¡i cay ná»“ng, báº¡n biáº¿t nhÃ  hÃ ng ThÃ¡i nÃ o náº¥u chuáº©n vá»‹ khÃ´ng?", "Cuá»‘i tuáº§n nÃ y mÃ¬nh muá»‘n Ä‘i Äƒn sÃ¡ng á»Ÿ má»™t quÃ¡n cÃ³ khÃ´ng gian sÃ¢n vÆ°á»n thoÃ¡ng Ä‘Ã£ng.", "cao láº§u há»™i an", "CÃ³ quÃ¡n Äƒn váº·t nÃ o má»Ÿ cá»­a khuya sau 10 giá» tá»‘i mÃ  giao hÃ ng nhanh khÃ´ng?", "mÃ³n trÃ¡ng miá»‡ng", "á» khu Äáº¡i há»c Cáº§n ThÆ¡ cÃ³ quÃ¡n Äƒn nÃ o ngon, bá»•, ráº» mÃ  buá»•i trÆ°a khÃ´ng quÃ¡ Ä‘Ã´ng khÃ´ng", "nÆ°á»›c Ã©p trÃ¡i cÃ¢y", "bÃ¡nh canh gháº¹", "MÃ¬nh thÃ¨m trÃ  sá»¯a trÃ¢n chÃ¢u Ä‘Æ°á»ng Ä‘en, quÃ¡n nÃ o lÃ m ngon nháº¥t á»Ÿ Cáº§n ThÆ¡", "sushi nháº­t báº£n", "bÃºn cháº£ hÃ  ná»™i", "sá»¯a chua trÃ¢n chÃ¢u", "quÃ¡n Äƒn gáº§n Ä‘Ã¢y", "quÃ¡n Äƒn ngon ráº»", "tokbokki hÃ n quá»‘c",        ]
        output_dir = "/home/minh/Documents/SEG_project/datas/queries_lable"
        
        # (QUAN TRá»ŒNG) XÃ³a táº¥t cáº£ cÃ¡c file káº¿t quáº£ cÅ©
        print(f"--- Äang xÃ³a cÃ¡c file .csv cÅ© trong: {output_dir} ---")
        for f in os.listdir(output_dir):
            if f.endswith('.csv'):
                os.remove(os.path.join(output_dir, f))
        print("--- ÄÃ£ xÃ³a file cÅ©. Báº¯t Ä‘áº§u táº¡o file káº¿t quáº£ Má»šI ---")

        for q in test_queries:
            search_results = engine.search(q)
            # print("------ Káº¾T QUáº¢ ------")
            if not search_results.empty:
                # print(search_results[['name', 'text_for_embedding', 'comments']])
                
                # ğŸ’¾ LÆ°u ra file CSV (ÄÃƒ Sá»¬A Lá»–I LOGIC)
                
                # Sá»­ dá»¥ng hÃ m chuáº©n hÃ³a Má»šI
                clean_filename_part = sanitize_query_for_filename(q)
                
                # TÃªn file KHÃ”NG CÃ“ "labeled_"
                output_file = f"{output_dir}/search_results_{clean_filename_part}.csv"
                
                search_results.to_csv(output_file, index=False, encoding='utf-8-sig')
                # print(f"âœ… ÄÃ£ lÆ°u káº¿t quáº£ truy váº¥n '{q}' vÃ o file: {output_file}")
                
            # print("="*20)
    except Exception as e:
        print(f"\nğŸ’¥ ÄÃ£ xáº£y ra lá»—i nghiÃªm trá»ng: {e}")
        
if __name__ == '__main__':
    main()